% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RerF.R
\name{RerF}
\alias{RerF}
\title{RerF forest Generator}
\usage{
RerF(X, Y, min.parent = 6L, trees = 100L, max.depth = 0L, bagging = 0.2,
  replacement = TRUE, stratify = FALSE, fun = NULL, mat.options = list(p
  = ifelse(is.null(cat.map), ncol(X), length(cat.map)), d =
  ceiling(sqrt(ncol(X))), random.matrix = "binary", rho =
  ifelse(is.null(cat.map), 1/ncol(X), 1/length(cat.map))),
  rank.transform = FALSE, store.oob = FALSE, store.ns = FALSE,
  progress = FALSE, rotate = F, num.cores = 0L, seed = 1L,
  cat.map = NULL)
}
\arguments{
\item{X}{an n by d numeric matrix (preferable) or data frame. The rows correspond to observations and columns correspond to features.}

\item{Y}{an n length vector of class labels.  Class labels must be integer or numeric and be within the range 1 to the number of classes.}

\item{min.parent}{the minimum splittable node size.  A node size < min.parent will be a leaf node. (min.parent = 6)}

\item{trees}{the number of trees in the forest. (trees=100)}

\item{max.depth}{the longest allowable distance from the root of a tree to a leaf node (i.e. the maximum allowed height for a tree).  If max.depth=0, the tree will be allowed to grow without bound.  (max.depth=0)}

\item{bagging}{a non-zero value means a random sample of X will be used during tree creation.  If replacement = FALSE the bagging value determines the percentage of samples to leave out-of-bag.  If replacement = TRUE the non-zero bagging value is ignored. (bagging=.2)}

\item{replacement}{if TRUE then n samples are chosen, with replacement, from X. (replacement=TRUE)}

\item{stratify}{if TRUE then class sample proportions are maintained during the random sampling.  Ignored if replacement = FALSE. (stratify = FALSE).}

\item{fun}{a function that creates the random projection matrix. If NULL and cat.map is NULL, then RandMat is used. If NULL and cat.map is not NULL, then RandMatCat is used, which adjusts the sampling of features when categorical features have been one-of-K encoded. If a custom function is to be used, then it must return a matrix in sparse representation, in which each nonzero is an array of the form (row.index, column.index, value). See RandMat or RandMatCat for details. (fun=NULL)}

\item{mat.options}{a list of parameters to be used by fun. (mat.options=c(ncol(X), round(ncol(X)^.5),1L, 1/ncol(X)))}

\item{rank.transform}{if TRUE then each feature is rank-transformed (i.e. smallest value becomes 1 and largest value becomes n) (rank.transform=FALSE)}

\item{store.oob}{if TRUE then the samples omitted during the creation of a tree are stored as part of the tree.  This is required to run OOBPredict(). (store.oob=FALSE)}

\item{store.ns}{if TRUE then the number of training observations at each node is stored. This is required to run FeatureImportance() (store.ns=FALSE)}

\item{progress}{if TRUE then a pipe is printed after each tree is created.  This is useful for large datasets. (progress=FALSE)}

\item{rotate}{if TRUE then the data matrix X is uniformly randomly rotated for each tree. (rotate=FALSE)}

\item{num.cores}{the number of cores to use while training. If num.cores=0 then 1 less than the number of cores reported by the OS are used. (num.cores=0)}

\item{seed}{the seed to use for training the forest. (seed=1)}

\item{cat.map}{a list specifying which columns in X correspond to the same one-of-K encoded feature. Each element of cat.map is a numeric vector specifying the K column indices of X corresponding to the same categorical feature after one-of-K encoding. All one-of-K encoded features in X must come after the numeric features. The K encoded columns corresponding to the same categorical feature must be placed contiguously within X. The reason for specifying cat.map is to adjust for the fact that one-of-K encoding cateogorical features results in a dilution of numeric features, since a single categorical feature is expanded to K binary features. If cat.map = NULL, then RerF assumes all features are numeric (i.e. none of the features have been one-of-K encoded).}
}
\value{
forest
}
\description{
Creates a decision forest based on an input matrix and class vector.  This is the main function in the rerf package.
}
\examples{
### Train RerF on numeric data ###
library(rerf)
forest <- RerF(as.matrix(iris[, 1:4]), iris[[5L]], num.cores = 1L)

### Train RerF on one-of-K encoded categorical data ###
df1 <- as.data.frame(Titanic)
nc <- ncol(df1)
df2 <- df1[NULL, -nc]
for (i in which(df1$Freq != 0L)) {
  df2 <- rbind(df2, df1[rep(i, df1$Freq[i]), -nc])
}
n <- nrow(df2) # number of observations
p <- ncol(df2) - 1L # number of features
num.categories <- apply(df2[, 1:p], 2, function(x) length(unique(x)))
p.enc <- sum(num.categories) # number of features after one-of-K encoding
X <- matrix(0, nrow = n, ncol = p.enc) # initialize training data matrix X
cat.map <- vector("list", p)
col.idx <- 0L
# one-of-K encode each categorical feature and store in X
for (j in 1:p) {
  cat.map[[j]] <- (col.idx + 1L):(col.idx + num.categories[j])
  X[, cat.map[[j]]] <- dummies::dummy(df2[[j]]) # converts categorical feature to K dummy variables
  col.idx <- col.idx + num.categories[j]
}
Y <- df2$Survived

# specifying the cat.map in RerF allows training to be aware of which dummy variables correspond
# to the same categorical feature
forest <- RerF(X, Y, num.cores = 1L, cat.map = cat.map)

### Train a random rotation ensemble of CART decision trees (see Blaser and Fryzlewicz 2016) ###
forest <- RerF(as.matrix(iris[, 1:4]), iris[[5L]], num.cores = 1L,
mat.options = list(4, 2, "rf", NULL), rotate = TRUE)

}
